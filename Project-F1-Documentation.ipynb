{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Grand Prix: Building and Visualizing Formula 1 Stats with REST API\n",
    "\n",
    "Searching for data sources to use for practicing or sharpening your skills could be tiresome. Data sources come in various forms but for this project I have focused on Rest API as a data source.I've often come accross or read articles that mention Rest APIs as a common source for retrieving data for exploration, but up until now, thanks to [Rapid API](https://rapidapi.com/) I  had never tried explored data with the source being an API. Here's my small little journey for a very basic process on how i ende up with some Formula 1 stats to explore. I hope you will find some useful tips along the way!\n",
    "\n",
    "Before we jump in make sure you have access to to the following, feel free to use any equivalent tool of your choice that will satisfy the same purpose as intended in this project:\n",
    "* Postman - went with Postamn to manage the API calls made to Rapid API but there are many other platforms that can do a similar job, feel free to use any of your choice.\n",
    "* S3 Bucket - if you have gone with AWS or any equivalent service depending on which cloud service provider you have gone with.\n",
    "* IAM Role with permissions to access S3 Bucket - necessary for executing jobs later, if you picked a different cloud service provider then the equivalence of this is also fine\n",
    "* An Account with Rapid API - this is a free sign up and it even allows you a reasonable amount of API calls per day \n",
    "* IDE or Data Warehouse Account -  Myql Workbench/PostgreSql/MSSQL/Snowflake or any other of your choice\n",
    "\n",
    "## Raw Data Architeture\n",
    "Here is a quick visual to better understand what sort of Formula 1 stats we will be collecting from the data source:\n",
    "![alt text](<05 archi-raw-f1.jpg>)\n",
    "## Collect and Upload to S3 Bucket\n",
    "### Phase 1 - Extracting Nested JSON Files From API \n",
    "Sign up with [Rapid API](https://rapidapi.com/) and go over to the marketplace and search for Formula 1. Pick the one made by API SPORTS.\n",
    "Head over to Postman and create GET and PUT requests to call the data from Rapid AI and push to your S3 Bucket.\n",
    "For the GET configuration make sure the headers provided by Rapid API are set up otherwise the request will not be returned.\n",
    "\n",
    "![alt text](<01 GET_postman_config.png>)\n",
    "\n",
    "Also ensure in the scripts tab on the *Post Response* section you put in this code ```javascript pm.environment.set(\"responseData\", JSON.stringify(pm.response.json()));```.\n",
    "This script saves the JSON response from the GET request to an environment variable called responseData. You will need the variable when setting up your PUT request.\n",
    "\n",
    "![alt text](<02 GET_postman_config2.png>)\n",
    "\n",
    "For the PUT request which is responsible for uploading your resukts from the GET request into the S3 Bucket, you would need to first create a [presigned URL](https://docs.aws.amazon.com/AmazonS3/latest/userguide/PresignedUrlUploadObject.html). You can choose a preferred method to do so as long as you use the generated URL in your PUT request it should work. I went wuth AWS SDK using Python as, here is an example of the script:\n",
    "\n",
    "![alt text](<03 presigned URL eample.png>)\n",
    "\n",
    "Another important piece to not miss out is adding the environment variable to the 'body' tab of your PUT request. If you miss this, you will upload a blank file into S3.\n",
    "\n",
    "![alt text](<04 PUT_postman_config.png>)\n",
    "\n",
    "If you already have a bucket set up with an IAM role, you should be good to go, if not, head to your AWS console and set it up. Important to not that you cannot generate a presigned URL if you do not have an existing bucket. \n",
    "\n",
    "Once all done run your requests in Postman starting with the GET request first then the PUT, this will upload the data into your bucket. Do this for all the APIs related to the data you require."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 2 - Transforming Nested JSON files to Parquet Format\n",
    "Once I had results uploaded into the S3 Bucket, the next stage was to convert the data so that it was usable for what I wanted to do with it. As it stood the results were stored in json format. You could chose to download one file or preview results of your files from Postman to get an understanding of how your json files are structured. In my case some of the files had objects that were the \"value\" element of another object. This is otherwise known as nested json files.\n",
    "\n",
    "To extract data that was nested with in another json object, i had to use Spark. Spark helped me to flatten the json nested json files by using a pyspak function called explode. Once that was done i was able to save the output in parquet format in a different bucket location to ensure that i have a seperate path to the raw data and the one that i had just processed.\n",
    "I have pasted an example script i used to convert on of the files below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql.functions import col, explode\n",
    "\n",
    "# # Initialize Spark session builder\n",
    "# spark_builder = SparkSession.builder.appName(\"MySparkSession\")\n",
    "\n",
    "# # Create the Spark session\n",
    "# spark = spark_builder.getOrCreate()\n",
    "\n",
    "# # Load JSON data from S3\n",
    "# df = spark.read.format(\"json\")\\\n",
    "#             .option(\"multiLine\", True)\\\n",
    "#             .option(\"header\", True)\\\n",
    "#             .option(\"inferschema\", True)\\\n",
    "#             .load(s3_input_path)\n",
    "\n",
    "# df.printSchema()\n",
    "\n",
    "# print(\"done\")\n",
    "# # Explode the 'response' array\n",
    "# exploded_df = df.withColumn(\"response\", explode(\"response\"))\n",
    "\n",
    "# # Select the nested fields explicitly\n",
    "# flattened_df = exploded_df.select(\n",
    "#     col(\"response.capacity\").alias(\"capacity\"),\n",
    "#     col(\"response.competition.id\").alias(\"competition_id\"),\n",
    "#     col(\"response.competition.location.city\").alias(\"competition_location_city\"),\n",
    "#     col(\"response.competition.location.country\").alias(\"competition_location_country\"),\n",
    "#     ...,\n",
    "#     col(\"response.length\").alias(\"length\"),\n",
    "#     col(\"response.name\").alias(\"name\")\n",
    "# )\n",
    "\n",
    "# flattened_df.show(truncate=False)\n",
    "\n",
    "\n",
    "# # Path to save the flattened data in S3\n",
    "# s3_output_path = \"s3a://your-s3-bucket-location/\"\n",
    "\n",
    "# # Write the flattened DataFrame to S3\n",
    "# flattened_df.write.format(\"parquet\").mode(\"overwrite\").save(s3_output_path)\n",
    "\n",
    "# print(\"complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 3 - Creating Tables from Parquet Files \n",
    "From the parquest files stored in a different S3 Bucket location, I now had the data in rows and colums format which meant i could now organise the data into tables. I used AWS GLue for this by firstly creating an AWS Crawler job. When you configuring teh crawler ensure you use the IAM role created dedicated for the job and that it also has permissions to access teh S3 Buckets. The Crawler ran through all the file paths where my parquest files were stored, processed the data to create schemas as intended. The output was as per the example below:\n",
    "\n",
    "![alt text](<06 Data Catalog Tables.png>)\n",
    "\n",
    "AWS has a service (AWS Athena) that allows you to have a look at the data and even query it once tables have been created in the Catalog. To access this straight was i just clicked on the Table Data link as ahown below and followed the prompt:\n",
    "\n",
    "![alt text](<07 Data Catalog Tables View Athena.png>)\n",
    "\n",
    "AWS Athena allwos you to even query your tables so you can have a quick look on what your processed files (from teh Crwler Job) look like. This is how one of my tables looked like:\n",
    "\n",
    "![alt text](<08 Athena to query Data.png>)\n",
    "\n",
    "All the tables looked good as expected. At this point you can choose to model how you want your tables to look like for example if there are seperate files for lets say different seasons, you could join the tables using some DDL queries and store your results in a clean fresh database which would be the database you then use to fetch your data from. Notice in the screenshot below on the left hand side how my tables have changed from the previous screenshot. I used some DDL commands in the `formula-1-database` which used tables that were stored in `formula-one` database. \n",
    "\n",
    "![alt text](<09 CTAS.png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 4 - Creating a RDS Instance and Mysql Connector\n",
    "With our tables ready for further exploration, the options from this point depend on which tool or cloud service you would want to use tha data from. This will determine the next steps. In my case i wanted to connect this data to Mysql Workbench and from there i would use any other visualization tool to access the data. So the first step I took was to create an RDS instance then connect Mysql Workbench to this RDS Instance using a Mysql Connector. You can find more connectors on AWS marketplace depending on the type of connection desired. \n",
    "\n",
    "* Create an RDS Instance \n",
    "* Create a Mysql connector \n",
    "\n",
    "Meanwhile in Mysql workbench i created the exact same tables as the tables in the `formula-1-databse` database. The reason for this is explained in the next point.\n",
    "\n",
    "As it stood i had tables from created from the first crawler i created plus the DDL queries i wrote only. In order to show the tables i created in Mysql Workbench, i created another crawler but this time configured the crawler to crawl the RDS. By doing so it identifies those new tables and gets me tables with data versus tables without data in the same enironment with the empty tables.\n",
    "I needed one more job, which would be an ETL job to copy data from `formula-1-database` to the empty tables in Mysql Workbench. To achieve this i used AWS Glue Studio.\n",
    "\n",
    "![alt text](<10 ETL Jobs.png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 5 - Getting Data from Database for Data Analysis\n",
    "Finally what was left was simply running some queires from the database to explore the data, create some views that gave results for desired metrics before fetching the data from a powerful visualization tool. I've shared the whole process overview in the architecture diagram below and also a sample visualization i did using Power BI.\n",
    "\n",
    "Thanks!\n",
    "\n",
    "![alt text](<11 Project Architecture.png>)\n",
    "\n",
    "<video controls src=\"14 F1_Media.mp4\" title=\"Title\"></video>\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
